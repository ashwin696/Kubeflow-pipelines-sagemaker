{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost Mnist classification pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install Sagemaker, kfp and boto3 sdk. \n",
    "\n",
    "> Note: Be sure to use specified KFP SDK version in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.13'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install kfp==1.8.13\n",
    "import kfp\n",
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.109'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install boto3==1.26.109\n",
    "import boto3\n",
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.145.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install sagemaker==2.146.0\n",
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker IAM Role creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to run this pipeline, we need two levels of IAM permissions. One is Sagemaker user and another is sagemaker execution role. Below are the steps to obtain all required permissions.\n",
    "Go to your cloud9 console or terminal with aws cli configured.  \n",
    "\n",
    "> a) Create a IAM user\n",
    "```bash\n",
    "aws iam create-user --user-name sagemaker-kfp-user\n",
    "aws iam attach-user-policy --user-name sagemaker-kfp-user --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\n",
    "```\n",
    "> b) Create an access key and export them as env variable  \n",
    "```bash\n",
    "aws iam create-access-key --user-name sagemaker-kfp-user > ~/environment/create_output.json\n",
    "export AWS_ACCESS_KEY_ID_VALUE=$(jq -j .AccessKey.AccessKeyId ~/environment/create_output.json | base64)\n",
    "export AWS_SECRET_ACCESS_KEY_VALUE=$(jq -j .AccessKey.SecretAccessKey ~/environment/create_output.json | base64)\n",
    "```\n",
    "> c) Create an IAM execution role for Sagemaker and S3 so that the job can assume this role in order to perform Sagemaker and S3 actions. Make a note of this role as you will need it during pipeline creation step\n",
    "```bash\n",
    "TRUST=\"{ \\\"Version\\\": \\\"2012-10-17\\\", \\\"Statement\\\": [ { \\\"Effect\\\": \\\"Allow\\\", \\\"Principal\\\": { \\\"Service\\\": \\\"sagemaker.amazonaws.com\\\" }, \\\"Action\\\": \\\"sts:AssumeRole\\\" } ] }\"\n",
    "aws iam create-role --role-name sagemaker-kfp-role --assume-role-policy-document \"$TRUST\"\n",
    "aws iam attach-role-policy --role-name sagemaker-kfp-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\n",
    "aws iam attach-role-policy --role-name sagemaker-kfp-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n",
    "aws iam get-role --role-name sagemaker-kfp-role --output text --query 'Role.Arn'\n",
    "\n",
    "```\n",
    "> d) Create Kubernetes secrets **aws-secret** with Sagemaker and S3 policies. Please make sure to create `aws-secret` in kubeflow user namespace.\n",
    "\n",
    "```bash\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: aws-secret\n",
    "  namespace: kubeflow-user-example-com\n",
    "type: Opaque\n",
    "data:\n",
    "  AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID_VALUE\n",
    "  AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY_VALUE\n",
    "EOF\n",
    "```\n",
    "> e) Letâ€™s assign sagemaker:InvokeEndpoint permission to Worker node IAM role\n",
    "```bash\n",
    "cat <<EoF > ~/environment/sagemaker-invoke.json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:InvokeEndpoint\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "EoF\n",
    "aws iam put-role-policy --role-name sagemaker-kfp-role --policy-name sagemaker-invoke-for-worker --policy-document file://~/environment/sagemaker-invoke.json\n",
    "```\n",
    "References: https://archive.eksworkshop.com/advanced/420_kubeflow/pipelines/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Kubeflow pipeline endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import time\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "from kfp.components import load_component_from_file, create_component_from_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialise Kubeflow pipeline client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crediantials are required if kubeflow is deployed for multi-tenancy\n",
    "credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path='/var/run/secrets/kubeflow/pipelines/token')\n",
    "client = kfp.Client(host=\"http://your-ml-pipeline-endpoint:8888\", credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/experiments/details/47062046-4693-4a98-be61-10ee9c024732\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': datetime.datetime(2023, 4, 17, 9, 18, 10, tzinfo=tzlocal()),\n",
       " 'description': 'Kubeflow pipeline with sagemaker',\n",
       " 'id': '47062046-4693-4a98-be61-10ee9c024732',\n",
       " 'name': 'sagemaker-kfp',\n",
       " 'resource_references': [{'key': {'id': 'kubeflow-user-example-com',\n",
       "                                  'type': 'NAMESPACE'},\n",
       "                          'name': None,\n",
       "                          'relationship': 'OWNER'}],\n",
       " 'storage_state': 'STORAGESTATE_AVAILABLE'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = 'sagemaker-kfp'\n",
    "\n",
    "client.create_experiment(name=experiment_name,\n",
    "                        description='Kubeflow pipeline with sagemaker',\n",
    "                        namespace='kubeflow-user-example-com')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Xgboost Model URI and Define the constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the uri of the XGBoost Model from AWS Public ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.5-1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "image_uri = image_uris.retrieve(framework='xgboost',region='us-east-1', version=\"1.5-1\")\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Defining the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = \"kfp-sagemaker-example\" \n",
    "S3_PIPELINE_PATH=f's3://{S3_BUCKET}/xgboost/pipeline_test'\n",
    "# Configure your Sagemaker execution role.\n",
    "SAGEMAKER_ROLE_ARN='arn:aws:iam::XXXXXXXXXXXX:role/sagemaker-kfp-role'\n",
    "PREFIX = \"xgboost/pipeline_test\"\n",
    "MNIST_DATA = \"mnist_data/mnist.pkl.gz\"\n",
    "XGBOOST_IMAGE = \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.5-1\"\n",
    "MODEL_NAME = 'Xgboost-mnist-' + time.strftime(\"%Y-%b-%d-%H-%M-%S\", time.gmtime())\n",
    "BASE_IMAGE = 'ashwin456/python:3.10-slim-boto3' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Sample Mnist data and Upload to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Data ingestion  \n",
    "\n",
    "  Download the Mnist data from this [link](https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz) and upload it s3 bucket so that the kfp component can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, numpy, urllib.request, json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Download the dataset\n",
    "#urllib.request.urlretrieve(\"https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\", \"mnist.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data_s3(file_path: str, s3_bucket: str, key: str):\n",
    "    import boto3\n",
    "    \n",
    " # if credentials are not in the environmnet, provide them manually\n",
    "    s3 = boto3.client('s3', region_name='us-east-1')\n",
    "    s3.create_bucket(Bucket=s3_bucket)\n",
    "\n",
    "    with open(file_path, 'rb') as data:\n",
    "        s3.upload_fileobj(data, s3_bucket, key)\n",
    "\n",
    "upload_data_s3(\"./mnist.pkl.gz\", S3_BUCKET, MNIST_DATA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define the function that fetches the mnist data from s3 and processes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(mnist_data:str, s3_bucket:str, prefix:str):\n",
    "    \"\"\"\n",
    "        Fetches the MNIST Data and processes it to produce train, test and validation set &\n",
    "        Uploads the prepared data to s3.\n",
    "        \n",
    "        Args:\n",
    "            mnist_data: str = Takes the file path in the s3 format \"folder/filename\"\n",
    "            s3_bucket:str = Name of the bucket\n",
    "            prefix: str = Name of the folder in which processed data should be uploaded\n",
    "            \n",
    "        Returns: None\n",
    "            \n",
    "    \"\"\"\n",
    "    #pending to clean up s3 download and upload code\n",
    "    import pickle\n",
    "    import gzip\n",
    "    import io\n",
    "    import boto3\n",
    "    import botocore\n",
    "    \n",
    "    # Downloading the data\n",
    "    session = boto3.Session()\n",
    "    s3 = boto3.resource(\"s3\", region_name='us-east-1')\n",
    "    mnist_file = mnist_data.split(sep='/')[1]\n",
    "    try:\n",
    "        s3.Bucket(s3_bucket).download_file(mnist_data, mnist_file)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            print(f\"The object does not exist at {url}.\")\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # loading \n",
    "    with gzip.open(mnist_file, \"rb\") as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = \"latin1\"\n",
    "        train_set, valid_set, test_set = u.load() \n",
    "\n",
    "    # converting data to libsvm format to get training compatibilty of xgboost ecr image\n",
    "    \n",
    "    partitions = [(\"train\", train_set), (\"validation\", valid_set), (\"test\", test_set)]\n",
    "    for partition_name, partition in partitions:\n",
    "        print(f\"{partition_name}: {partition[0].shape} {partition[1].shape}\")\n",
    "        labels = [t.tolist() for t in partition[1]]\n",
    "        vectors = [t.tolist() for t in partition[0]]\n",
    "        num_partition = 5  # partition file into 5 parts\n",
    "        partition_bound = int(len(labels) / num_partition)\n",
    "        for i in range(num_partition):\n",
    "            f = io.BytesIO()\n",
    "            lab = labels[i * partition_bound : (i + 1) * partition_bound]\n",
    "            val = vectors[i * partition_bound : (i + 1) * partition_bound]\n",
    "            f.write(\n",
    "                bytes(\n",
    "                    \"\\n\".join(\n",
    "                        [\"{} {}\".format(label, \" \".join([\"{}:{}\".format(i + 1, el) for i, el in enumerate(vec)]))\n",
    "                            for label, vec in zip(lab, val)]\n",
    "                    ),\n",
    "                    \"utf-8\",\n",
    "                )\n",
    "            )\n",
    "            f.seek(0)\n",
    "            key = f\"{prefix}/{partition_name}/examples{str(i)}\"\n",
    "            url = f\"s3://{s3_bucket}/{key}\"\n",
    "            print(f\"Writing to {url}\")\n",
    "            # uploading the converted file to s3\n",
    "            session.resource(\"s3\").Bucket(s3_bucket).Object(key).upload_fileobj(f)\n",
    "            print(f\"Done writing to {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters specific to XGBoost Model\n",
    "hyperparameters = {\n",
    "                   \"eta\": \"0.2\", \n",
    "                   \"gamma\": \"4\", \n",
    "                   \"max_depth\": \"5\", \n",
    "                   \"min_child_weight\": \"6\", \n",
    "                   \"num_class\": \"10\", \n",
    "                   \"num_round\": \"10\", \n",
    "                   \"objective\": \"multi:softmax\", \n",
    "                   \"verbosity\": \"0\"\n",
    "    }\n",
    "\n",
    "# specifing the channels(data) for training \n",
    "train_channels =[\n",
    "    {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"s3://{S3_BUCKET}/{PREFIX}/train/\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"s3://{S3_BUCKET}/{PREFIX}/validation/\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Sagemaker components and initialise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Creating reusable components from kubeflow pipeline sagemaker. Download from here \n",
    "[sagemaker-components](https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O train.yaml https://github.com/kubeflow/pipelines/blob/master/components/aws/sagemaker/TrainingJob/component.yaml\n",
    "#!wget -O create_model.yaml https://github.com/kubeflow/pipelines/blob/master/components/aws/sagemaker/model/component.yaml \n",
    "#!wget -O deploy.yaml https://github.com/kubeflow/pipelines/blob/master/components/aws/sagemaker/deploy/component.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_train_op = load_component_from_file(\"./train.yaml\")\n",
    "sagemaker_model_op = load_component_from_file(\"./create_model.yaml\")\n",
    "sagemaker_deploy_op = load_component_from_file(\"./deploy.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Initialise the components of our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data_op = create_component_from_func(func=prepare_data, base_image=BASE_IMAGE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create pipeline defining all the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='MNIST Classification pipeline',\n",
    "    description='MNIST Classification using XGBoost in SageMaker'\n",
    ")\n",
    "def xgboost_mnist_classification(\n",
    "    endpoint_url='',\n",
    "    region='us-east-1',\n",
    "    network_isolation=\"True\"\n",
    "):\n",
    "\n",
    "    prepare_data_task = prepare_data_op(\n",
    "        MNIST_DATA, \n",
    "        S3_BUCKET, \n",
    "        PREFIX\n",
    "    )\n",
    "    \n",
    "    training_task = sagemaker_train_op(\n",
    "        region=region,\n",
    "        endpoint_url=\"\",\n",
    "        job_name=MODEL_NAME,\n",
    "        image=XGBOOST_IMAGE,\n",
    "        training_input_mode=\"File\",\n",
    "        hyperparameters=hyperparameters,\n",
    "        channels=train_channels,\n",
    "        instance_type=\"ml.m4.xlarge\",\n",
    "        instance_count=\"1\",\n",
    "        volume_size=\"6\",\n",
    "        max_run_time=\"3600\",\n",
    "        model_artifact_path=S3_PIPELINE_PATH + \"/output\",\n",
    "        output_encryption_key=\"\",\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=\"False\",\n",
    "        spot_instance=\"False\",\n",
    "        max_wait_time=\"3600\",\n",
    "        role=SAGEMAKER_ROLE_ARN,\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "    \n",
    "    training_task.after(prepare_data_task)\n",
    "    \n",
    "    create_model_task = sagemaker_model_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name=training_task.outputs['job_name'],\n",
    "        image=training_task.outputs['training_image'],\n",
    "        model_artifact_url=training_task.outputs['model_artifact_url'],\n",
    "        network_isolation=network_isolation,\n",
    "        vpc_subnets=\"\",\n",
    "        vpc_security_group_ids=\"\",\n",
    "        role=SAGEMAKER_ROLE_ARN\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    sagemaker_deploy = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name_1=create_model_task.output\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and run the the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Compile your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(xgboost_mnist_classification, 'xgboost.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Deploy your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/experiments/details/47062046-4693-4a98-be61-10ee9c024732\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/runs/details/74bf384b-6387-4b83-99e1-f043068ef8aa\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# client = kfp.Client()\n",
    "aws_experiment = client.create_experiment(name=experiment_name)\n",
    "my_run = client.run_pipeline(aws_experiment.id, 'Xgboost-mnist', \n",
    "  'xgboost.zip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our endpoint with sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [8], Predicted: 8.0\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip, numpy, urllib.request, json\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "# Replace the endpoint name with yours.\n",
    "ENDPOINT_NAME='Endpoint20230418182829-1YRI'\n",
    "ENDPOINT_CONFIG=\"EndpointConfig20230418182829-1YRI\"\n",
    "# We will use the same dataset that was downloaded at the beginning of the notebook.\n",
    "\n",
    "# Simple function to create a csv from our numpy array\n",
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    numpy.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()\n",
    "\n",
    "runtime = boto3.Session(region_name='us-east-1').client('sagemaker-runtime')\n",
    "\n",
    "payload = np2csv(train_set[0][30:31])\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                   ContentType='text/csv',\n",
    "                                   Body=payload)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(f\"Input: {train_set[1][31:32]}, Predicted: {result}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up \n",
    "\n",
    "#### Delete the sagemaker endpoint, endpoint_config, and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.Session(region_name='us-east-1').client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4a07b19c-d00e-4a0e-9f6e-b7c65ad6a949',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4a07b19c-d00e-4a0e-9f6e-b7c65ad6a949',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 18 Apr 2023 18:38:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_endpoint_config(EndpointConfigName=ENDPOINT_CONFIG)\n",
    "sm.delete_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "sm.delete_model(ModelName=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export S3_BUCKET=S3_BUCKET\n",
    "!aws s3 rb s3://$S3_BUCKET --force"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
